<!DOCTYPE HTML>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Deep Generative Dual Memory Network for Continual Learning">
	<meta name="author" content="Nitin Kamra">
	<title>Deep Generative Dual Memory Network for Continual Learning</title>
	<link href="../../stylesheet.css" rel="stylesheet" type="text/css">
	<link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
		type="text/css">
	<script type="text/javascript" async
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>

<body id="page-top">
	<table
		style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tbody>
			<tr style="padding:0px">
				<td style="padding:0px">
					<!-- Navigation -->
					<p style="text-align: center; padding: 20px;">
						<a href="../../index.html#dgdmn">Back</a>
					</p>

					<!-- Title -->
					<h1>Deep Generative Dual Memory Network for Continual Learning</h1>

					<!-- Authors -->
					<p style="text-align: center; font-size: 1.2em;"><strong>Nitin Kamra, Umang Gupta, Yan Liu</strong>
					</p>
					<p style="text-align: center; font-size: small;">Posted on November 27, 2017</p>

					<!-- TL;DR -->
					<p align="justify"><strong>TL;DR:</strong> A dual memory architecture inspired from human brain to
						learn sequentially incoming tasks, while averting catastrophic forgetting.</p>

					<!-- Intro -->
					<p align="justify">This post summarizes our paper <a href="https://arxiv.org/abs/1710.10368">Deep
							Generative Dual Memory Network for Continual Learning</a>. We present a method to learn
						tasks sequentially without forgetting previous ones, inspired by the human brain.</p>

					<p style="text-align: center;">
						<img src="img/humanbrain.jpg" alt="The Human Brain" width=300>
					</p>
					<p style="text-align: center; font-size:10px;">
						Image credits: http://www.sciencecodex.com/files/040215%20Modular%20brains.jpg
					</p>

					<!-- Catastrophic Forgetting -->
					<h3>Catastrophic Forgetting</h3>

					<p align="justify">Neural networks suffer from <i>catastrophic forgetting</i>â€”they forget previously
						learned knowledge when trained on new tasks. To solve this, a system must be able to learn
						continuously from incoming data without needing to store and retrain on all past data.</p>

					<p align="justify">Neuroscientific evidence suggests the human brain avoids this using
						<strong>Complementary Learning Systems</strong> (hippocampus for rapid learning, neocortex for
						long-term consolidation) and <strong>Experience Replay</strong> (consolidating memories during
						sleep). We mimic this structure to enable continual learning in AI.
					</p>

					<!-- Formal Setup -->
					<h3>Formal Setup</h3>

					<p align="justify">We address <i>sequential multitask learning</i>, where a model learns a sequence
						of tasks \(\mathbb{T}\) one by one. The goal is to minimize test loss on all tasks seen so far,
						assuming a <strong>finite memory</strong> (\(N_{max}\)) that prevents storing all training
						examples.</p>

					<!-- DGDMN -->
					<h3>Deep Generative Dual Memory Network (DGDMN)</h3>

					<p align="justify">We propose the DGDMN, a dual-memory model comprising:</p>
					<ul>
						<li><strong>Short-Term Memory (STM):</strong> Modeled after the hippocampus. It consists of
							small, task-specific Deep Generative Memories (STTMs) that adapt quickly to new tasks.</li>
						<li><strong>Long-Term Memory (LTM):</strong> Modeled after the neocortex. It is a large Deep
							Generative Memory that consolidates knowledge from the STM.</li>
					</ul>

					<p style="text-align: center;">
						<img src="img/dgdmn.png" alt="Deep Generative Dual Memory Network" width=1100
							style="max-width:100%;">
					</p>
					<p style="text-align: center; font-size:14px;">Figure 2: Deep Generative Dual Memory Network</p>

					<p align="justify"><strong>Generative Experience Replay:</strong> Instead of storing raw samples, we
						use generative models (VAEs) to store and replay experiences. This allows us to generate
						representative samples of past tasks within a fixed memory budget.</p>

					<p align="justify"><strong>The Learning Cycle:</strong>
					<ol>
						<li>New tasks are learned quickly in the <strong>STM</strong>.</li>
						<li>During "sleep" periods (when STM is full), the STM generates samples of its tasks.</li>
						<li>These samples are transferred to the <strong>LTM</strong>, which consolidates them with its
							existing knowledge via generative replay.</li>
					</ol>
					This periodic consolidation allows for fast learning while preserving long-term knowledge, mirroring
					human memory consolidation.</p>

					<!-- Results -->
					<h3>Results</h3>

					<p align="justify">We evaluated DGDMN on sequential image classification tasks (Permnist, Digits,
						TDigits) against baselines like EWC and Pseudopattern Rehearsal (PPR).</p>

					<!-- Figure 3 Table -->
					<table style="width:100%; text-align:center;">
						<tr>
							<td style="padding:10px;">
								<img src="img/permnistacc_PPR.png" style="max-width:100%;">
								<br>(a) PPR on Permnist
							</td>
							<td style="padding:10px;">
								<img src="img/permnistacc_EWC.png" style="max-width:100%;">
								<br>(b) EWC on Permnist
							</td>
							<td style="padding:10px;">
								<img src="img/permnistacc_DGDMN.png" style="max-width:100%;">
								<br>(c) DGDMN on Permnist
							</td>
						</tr>
						<tr>
							<td style="padding:10px;">
								<img src="img/digitsacc_PPR.png" style="max-width:100%;">
								<br>(d) PPR on Digits
							</td>
							<td style="padding:10px;">
								<img src="img/digitsacc_EWC.png" style="max-width:100%;">
								<br>(e) EWC on Digits
							</td>
							<td style="padding:10px;">
								<img src="img/digitsacc_DGDMN.png" style="max-width:100%;">
								<br>(f) DGDMN on Digits
							</td>
						</tr>
					</table>
					<p style="text-align: center; font-size:14px;">Figure 3: Accuracy curves. DGDMN (c, f) retains
						performance on previous tasks significantly better than baselines.</p>

					<p align="justify"><strong>Performance:</strong> DGDMN consistently outperforms baselines, retaining
						higher average accuracy on previous tasks. Unlike EWC which stagnates, or standard networks
						which catastrophically forget, DGDMN's dual memory allows it to learn new tasks while preserving
						old ones (Figure 3 & 4).</p>

					<!-- Figure 4 Table -->
					<table style="width:100%; text-align:center;">
						<tr>
							<td style="padding:10px;">
								<img src="img/permnist_avgforget.png" style="max-width:100%;">
								<br>(a) Permnist
							</td>
							<td style="padding:10px;">
								<img src="img/digits_avgforget.png" style="max-width:100%;">
								<br>(b) Digits
							</td>
						</tr>
					</table>
					<p style="text-align: center; font-size:14px;">Figure 4: Average forgetting curves. DGDMN maintains
						higher average accuracy as more tasks are added.</p>

					<p align="justify"><strong>Long Sequences and Training Time:</strong> On a longer sequence of 40
						tasks (TDigits), DGDMN shows much more gradual
						forgetting compared to a single Deep Generative Replay (DGR) model. Crucially, <strong>DGDMN is
							an order of magnitude faster</strong> to train (Figure 5c) because it consolidates to LTM
						only periodically, rather than after every task.</p>

					<!-- Figure 5 Table -->
					<table style="width:100%; text-align:center;">
						<tr>
							<td style="padding:10px;">
								<img src="img/tdigits_avgforget.png" style="max-width:100%;">
								<br>(a) Avg accuracy (all tasks)
							</td>
							<td style="padding:10px;">
								<img src="img/tdigits_avgforget_10.png" style="max-width:100%;">
								<br>(b) Avg accuracy (last 10)
							</td>
							<td style="padding:10px;">
								<img src="img/tdigits_time.png" style="max-width:100%;">
								<br>(c) Training time
							</td>
						</tr>
					</table>
					<p style="text-align: center; font-size:14px;">Figure 5: DGDMN vs DGR on 40 tasks. DGDMN forgets
						slower and trains much faster.</p>

					<p align="justify"><strong>Robustness:</strong> Using VAEs provides resilience to noise and
						occlusion. The reconstruction step acts as a denoiser, allowing the model to recognize corrupted
						inputs better than standard networks (Figure 6).</p>

					<!-- Figure 6 Table -->
					<table style="width:100%; text-align:center;">
						<tr>
							<td style="padding:10px;">
								<img src="img/noise_occlude_viz.png" style="max-width:100%;">
								<br>(a) Reconstruction
							</td>
							<td style="padding:10px;">
								<img src="img/noise_plot.png" style="max-width:100%;">
								<br>(b) vs. Noise
							</td>
							<td style="padding:10px;">
								<img src="img/occlusion_plot.png" style="max-width:100%;">
								<br>(c) vs. Occlusion
							</td>
						</tr>
					</table>
					<p style="text-align: center; font-size:14px;">Figure 6: Robustness to noise and occlusion.</p>

					<h3>Conclusion</h3>

					<p align="justify">DGDMN successfully emulates the human brain's dual memory system to learn
						sequentially without catastrophic forgetting. By combining an STM for fast learning and an LTM
						for consolidated knowledge, supported by generative replay, it offers a robust and efficient
						solution for continual learning.</p>

					<h3>References</h3>

					<p align="justify">
						<strong>[Kirkpatrick <i>et al.</i>, 2017]</strong> Overcoming catastrophic forgetting in neural
						networks. <i>PNAS</i>.<br>
						<strong>[Benna and Fusi, 2016]</strong> Computational principles of synaptic memory
						consolidation. <i>Nature neuroscience</i>.<br>
						<strong>[Goodfellow <i>et al.</i>, 2015]</strong> An empirical investigation of catastrophic
						forgetting in gradient-based neural networks. <i>arXiv</i>.<br>
						<strong>[Yang <i>et al.</i>, 2014]</strong> Sleep promotes branch-specific formation of
						dendritic spines after learning. <i>Science</i>.<br>
						<strong>[O'Neill <i>et al.</i>, 2010]</strong> Play it again: reactivation of waking experience
						and memory. <i>Trends in neurosciences</i>.<br>
						<strong>[Robins, 2004]</strong> Sequential learning in neural networks: A review and a
						discussion of pseudorehearsal based methods. <i>Intelligent Data Analysis</i>.<br>
						<strong>[McClelland <i>et al.</i>, 1995]</strong> Why there are complementary learning systems
						in the hippocampus and neocortex. <i>Psychological review</i>.<br>
					</p>
				</td>
			</tr>
		</tbody>
	</table>

</body>

</html>